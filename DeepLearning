import torch
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from torch.utils.data import TensorDataset, DataLoader
from SVMAndKNN import constructDataset
from torch import nn, optim
import torch.nn.functional as F
from keras_preprocessing import sequence
import matplotlib.pyplot as plt
'''
神经网络暂时采用比较简单的三层全连接层搭建，激活函数使用Relu，训练时容易出现过拟合所以加了0.4的Dropout
训练结果有0.92左右的准确率，召回率暂时还没加上
'''
class Classifier(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(256, 64)
        self.fc2 = nn.Linear(64, 32)
        self.fc3 = nn.Linear(32, 25)
        # 构造Dropout方法，在每次训练过程中都随机“掐死”百分之二十的神经元，防止过拟合。
        self.dropout = nn.Dropout(p=0.4)
    def forward(self, x):
        # 在训练过程中对隐含层神经元的正向推断使用Dropout方法
        x = self.dropout(F.relu(self.fc1(x)))
        x = self.dropout(F.relu(self.fc2(x)))
        # 在输出单元不需要使用Dropout方法
        x = F.log_softmax(self.fc3(x), dim=1)
        return x

if __name__ == "__main__":
    #将分类标签存下来
    labellist = ['3','2309','2853','407','44','4401','4402','4403','4405','4406','4407','4408','4409','4410',
                 '4411','4412','4413','4414','4415','4416','4418','4419','4420','4421','4702']
    l1, l2, l3, l4 = constructDataset('data/test.xlsx', '货名', '二类代码', 'All',labellist)
    # 可能是数据集太大，转换成向量后再转换为Tensor时内存不够，在这里加上最大特征数限制，可能影响精度
    countvec = CountVectorizer(max_features=256)
    x_train = countvec.fit_transform(l1)
    y_train = l2
    x_test = countvec.transform(l3)
    y_test = l4
    #先转换成数组再转换成长度相同的tensor
    x_train_tensor = torch.Tensor(sequence.pad_sequences(x_train.toarray()))
    y_train_tensor = torch.Tensor(y_train).long()
    x_test_tensor  = torch.Tensor(sequence.pad_sequences(x_test.toarray()))
    y_test_tensor  = torch.Tensor(y_test).long()
    #使用TensorDataset方法构建数据集
    dataset =TensorDataset(x_train_tensor,y_train_tensor)
    trainloader = DataLoader(dataset , batch_size=8,drop_last=True,shuffle=True)
    testset =TensorDataset(x_test_tensor,y_test_tensor)
    testloader = DataLoader(testset , batch_size=8,shuffle=True)

    #对上面定义的Classifier类进行实例化
    model = Classifier()
    # 定义损失函数交叉熵损失函数
    criterion = nn.CrossEntropyLoss()
    # 优化方法为Adam梯度下降方法，学习率为0.003
    optimizer = optim.Adam(model.parameters(), lr=0.003,weight_decay=0.00001)
    # 对训练集的全部数据学习15遍，这个数字越大，训练时间越长
    epochs = 15
    # 将每次训练的训练误差和测试误差存储在这两个列表里，后面绘制误差变化折线图用
    train_losses, test_losses = [], []
    print('开始训练')
    for e in range(epochs):
        running_loss = 0
        for images, labels in trainloader:
            # 将优化器中的求导结果都设为0，否则会在每次反向传播之后叠加之前的
            optimizer.zero_grad()
            # 进行推断，计算损失函数，反向传播优化权重，将损失求和
            log_ps = model(images)
            loss = criterion(log_ps, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()
        # 每次学完一遍数据集，都进行以下测试操作
        else:
            test_loss = 0
            accuracy = 0
            # 测试的时候不需要开自动求导和反向传播
            with torch.no_grad():
                # 关闭Dropout
                model.eval()
                # 对测试集中的所有都过一遍
                for data, labels in testloader:
                    # 对传入的测试集进行正向推断、计算损失，accuracy为测试集中模型预测正确率
                    log_ps = model(data)
                    test_loss += criterion(log_ps, labels.long())
                    ps = torch.exp(log_ps)
                    top_p, top_class = ps.topk(1, dim=1)
                    equals = top_class == labels.view(*top_class.shape)
                    accuracy += torch.mean(equals.type(torch.FloatTensor))
            # 恢复Dropout
            model.train()
            # 将训练误差和测试误差存在两个列表里，后面绘制误差变化折线图用
            train_losses.append(running_loss / len(trainloader))
            test_losses.append(test_loss / len(testloader))
            print("训练集学习次数: {}/{}.. ".format(e + 1, epochs),
                  "训练误差: {:.3f}.. ".format(running_loss / len(trainloader)),
                  "测试误差: {:.3f}.. ".format(test_loss / len(testloader)),
                  "模型分类准确率: {:.3f}".format(accuracy / len(testloader)))
    plt.plot(train_losses, label='Training loss')
    plt.plot(test_losses, label='test loss')
    plt.legend()
    plt.show()

